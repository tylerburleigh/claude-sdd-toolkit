"""
Integration tests for comparison view rendering.

Tests Rich TUI rendering for fidelity review, plan review, and status report
comparison views with side-by-side layouts, tables, and panels.
"""

import pytest
import json
from io import StringIO
from unittest.mock import patch, MagicMock

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from claude_skills.sdd_fidelity_review.report import FidelityReport
from claude_skills.sdd_fidelity_review.consultation import FidelityVerdict


# =============================================================================
# Fixtures for Mock Data
# =============================================================================


@pytest.fixture
def mock_spec_data():
    """Create mock spec data for testing."""
    return {
        "spec_id": "test-spec-001",
        "title": "Test Specification",
        "hierarchy": {
            "task-1": {
                "type": "task",
                "title": "Implement authentication service",
                "status": "completed",
                "metadata": {
                    "file_path": "src/auth/service.ts",
                    "requirements": [
                        "JWT token generation",
                        "Password hashing with bcrypt",
                        "Session management"
                    ]
                }
            },
            "task-2": {
                "type": "task",
                "title": "Add input validation",
                "status": "completed",
                "metadata": {
                    "file_path": "src/utils/validators.ts",
                    "requirements": [
                        "Email format validation",
                        "Password strength check"
                    ]
                }
            }
        }
    }


@pytest.fixture
def mock_fidelity_results():
    """Create mock fidelity review results with deviations."""
    return {
        "task-1": {
            "status": "pass",
            "deviations": [],
            "findings": "Authentication service correctly implements JWT tokens and bcrypt hashing."
        },
        "task-2": {
            "status": "partial",
            "deviations": [
                {
                    "requirement": "Password strength check",
                    "expected": "Minimum 8 characters, uppercase, lowercase, number, special char",
                    "actual": "Only checks minimum 6 characters",
                    "severity": "medium",
                    "recommendation": "Strengthen password validation rules"
                }
            ],
            "findings": "Input validation partially implemented, missing some password strength checks."
        }
    }


@pytest.fixture
def mock_parsed_ai_responses():
    """Create mock parsed AI responses for comparison view."""
    return [
        {
            "model": "gemini",
            "verdict": "pass",
            "issues": [],
            "recommendations": ["Add rate limiting", "Implement 2FA support"],
            "confidence": "high"
        },
        {
            "model": "codex",
            "verdict": "partial",
            "issues": [
                {
                    "severity": "medium",
                    "description": "Password validation too weak"
                }
            ],
            "recommendations": ["Strengthen password rules"],
            "confidence": "medium"
        }
    ]


@pytest.fixture
def mock_categorized_issues():
    """Create mock categorized issues for consensus matrix testing."""
    return [
        {
            "issue": "Missing input sanitization in user registration endpoint",
            "severity": "critical",
            "category": "security",
            "agreed_by": ["gemini", "codex", "claude"],
            "agreement_count": 3
        },
        {
            "issue": "Password validation requirements too weak",
            "severity": "high",
            "category": "security",
            "agreed_by": ["codex", "claude"],
            "agreement_count": 2
        },
        {
            "issue": "Missing error handling in authentication flow",
            "severity": "medium",
            "category": "reliability",
            "agreed_by": ["gemini"],
            "agreement_count": 1
        }
    ]


# =============================================================================
# Test: Fidelity Review Side-by-Side Comparison
# =============================================================================


def test_fidelity_review_side_by_side_comparison(
    mock_spec_data,
    mock_fidelity_results,
    mock_parsed_ai_responses
):
    """
    Test that fidelity review side-by-side comparison renders correctly.

    Verifies:
    - Model comparison table is created with Rich Table
    - Verdicts are displayed side-by-side across multiple models
    - Issue counts are shown for each model
    - Recommendations are formatted correctly
    - Color coding is applied (green=PASS, red=FAIL, yellow=PARTIAL)
    """
    # Create FidelityReport with mock data in expected format
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "partial",
            "agreement_rate": 0.5,
            "consensus_issues": [],
            "consensus_recommendations": []
        },
        "categorized_issues": [],
        "parsed_responses": mock_parsed_ai_responses,
        "models_consulted": len(mock_parsed_ai_responses)
    }

    report = FidelityReport(review_results)

    # Capture console output
    console = Console(file=StringIO(), width=100, legacy_windows=False)

    # Call the internal method that renders comparison table
    report._print_model_comparison_table(console)

    # Get rendered output
    output = console.file.getvalue()

    # Verify comparison table header
    assert "MODEL RESPONSE COMPARISON" in output
    assert "Side-by-side comparison of all AI model assessments" in output

    # Verify model columns are created
    assert "Model 1" in output
    assert "Model 2" in output

    # Verify verdicts are displayed
    assert "Verdict" in output
    assert "PASS" in output or "pass" in output.lower()
    assert "PARTIAL" in output or "partial" in output.lower()

    # Verify issue count row
    assert "Issue Count" in output or "Issues" in output

    # Verify recommendations are present
    # (The output includes recommendation counts or summaries)

    print("\n--- Comparison View Output ---")
    print(output)
    print("--- End Output ---\n")


def test_fidelity_review_no_models(mock_spec_data, mock_fidelity_results):
    """Test comparison view handles case with no AI model responses."""
    # Create report with no parsed responses
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {},
        "categorized_issues": [],
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)

    # Should not crash when printing comparison
    console = Console(file=StringIO(), width=100)
    report._print_model_comparison_table(console)

    # Output should be minimal or empty
    output = console.file.getvalue()
    assert "MODEL RESPONSE COMPARISON" not in output or output.strip() == ""


# =============================================================================
# Test: Consensus Matrix Display
# =============================================================================


def test_consensus_matrix_display(
    mock_parsed_ai_responses,
    mock_categorized_issues
):
    """
    Test that consensus matrix renders correctly with Rich Table.

    Verifies:
    - Consensus matrix table is created with issue rows and model columns
    - Each model column shows checkmark (âœ“) or dash (â€”) for agreement
    - Agreement percentage is calculated and displayed
    - Issues are color-coded by severity (critical=red, high=yellow, medium=blue, low=cyan)
    - Matrix shows which models identified each issue
    """
    # Create FidelityReport with mock data
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "partial",
            "agreement_rate": 0.67
        },
        "categorized_issues": mock_categorized_issues,
        "parsed_responses": mock_parsed_ai_responses,
        "models_consulted": len(mock_parsed_ai_responses)
    }

    report = FidelityReport(review_results)

    # Capture console output
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # Call the consensus matrix rendering method
    report._print_consensus_matrix(console, mock_categorized_issues)

    # Get rendered output
    output = console.file.getvalue()

    # Verify consensus matrix header
    assert "CONSENSUS MATRIX" in output
    assert "Shows which AI models identified each issue" in output

    # Verify column headers
    assert "Issue" in output
    assert "M1" in output  # Model 1 column
    assert "M2" in output  # Model 2 column
    assert "Agreement" in output

    # Verify issues are present (at least some text from issues)
    assert "sanitization" in output or "validation" in output or "error" in output

    # Verify agreement indicators (checkmarks or dashes)
    assert "âœ“" in output or "â€”" in output

    # Verify agreement percentages are shown
    assert "%" in output

    print("\n--- Consensus Matrix Output ---")
    print(output)
    print("--- End Output ---\n")


def test_consensus_matrix_no_issues(mock_parsed_ai_responses):
    """Test consensus matrix handles case with no categorized issues."""
    # Create report with no issues
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {},
        "categorized_issues": [],
        "parsed_responses": mock_parsed_ai_responses,
        "models_consulted": len(mock_parsed_ai_responses)
    }

    report = FidelityReport(review_results)

    # Should not crash when printing matrix with no issues
    console = Console(file=StringIO(), width=120)
    report._print_consensus_matrix(console, [])

    # Output should have header but no issue rows
    output = console.file.getvalue()
    # Matrix might still print header or be empty
    # The important thing is it doesn't crash


# =============================================================================
# Test: Issue Severity Panel Rendering
# =============================================================================


def test_issue_severity_panel_rendering(mock_categorized_issues, capsys):
    """
    Test that issue severity panels render correctly with Rich Panel components.

    Verifies:
    - Issues grouped by severity (critical, high, medium, low)
    - Each severity has its own color-coded panel (red, yellow, blue, cyan)
    - Panel titles include severity name and count
    - Panel icons are displayed (ðŸ”´, ðŸŸ¡, ðŸ”µ, âšª)
    - Issues are displayed as bullet points within panels
    - Border styles match severity colors
    """
    # Create FidelityReport with categorized issues
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "partial",
            "agreement_rate": 0.67,
            "consensus_recommendations": ["Improve input validation"]
        },
        "categorized_issues": mock_categorized_issues,
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)

    # Call print_console_rich() which prints to stdout
    # We'll capture it with capsys
    report.print_console_rich(verbose=False)

    # Capture stdout
    captured = capsys.readouterr()
    output = captured.out

    # Verify report header
    assert "IMPLEMENTATION FIDELITY REVIEW" in output
    assert "test-spec-001" in output

    # Verify consensus verdict is shown
    assert "Consensus Verdict" in output
    assert "Agreement Rate" in output

    # Verify severity panel titles with icons
    assert "CRITICAL ISSUES" in output or "critical" in output.lower()
    assert "HIGH" in output or "high" in output.lower()
    assert "MEDIUM" in output or "medium" in output.lower()

    # Verify issue content is present
    assert "sanitization" in output or "validation" in output or "error" in output

    # Verify bullet points for issues
    assert "â€¢" in output


def test_issue_severity_panel_empty(capsys):
    """Test severity panels handle case with no issues."""
    # Create report with no categorized issues
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "pass",
            "agreement_rate": 1.0,
            "consensus_recommendations": []
        },
        "categorized_issues": [],
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)

    # Should not crash when rendering with no issues
    report.print_console_rich(verbose=False)

    # Capture stdout
    captured = capsys.readouterr()
    output = captured.out

    # Verify header still renders
    assert "IMPLEMENTATION FIDELITY REVIEW" in output

    # Verify verdict is shown even without issues
    assert "Consensus Verdict" in output
    assert "PASS" in output or "pass" in output.lower()


# =============================================================================
# Test: Plan Review Comparison Tables
# =============================================================================


@pytest.fixture
def mock_plan_review_responses():
    """Create mock plan review responses for comparison testing."""
    return [
        {
            "tool": "gemini",
            "raw_review": "Plan review content from gemini",
            "overall_score": 8.5,
            "recommendation": "APPROVE",
            "dimensions": {
                "completeness": {"score": 9, "notes": "Well-structured phases"},
                "feasibility": {"score": 8, "notes": "Realistic timeline"},
                "clarity": {"score": 8, "notes": "Clear task descriptions"}
            },
            "issues": [
                {
                    "severity": "LOW",
                    "title": "Add more test coverage requirements"
                }
            ],
            "strengths": [
                "Well-defined phases",
                "Clear dependencies"
            ],
            "recommendations": [
                "Add performance benchmarks",
                "Include rollback plan"
            ]
        },
        {
            "tool": "codex",
            "raw_review": "Plan review content from codex",
            "overall_score": 7.5,
            "recommendation": "REVISE",
            "dimensions": {
                "completeness": {"score": 7, "notes": "Missing error handling"},
                "feasibility": {"score": 8, "notes": "Timeline is achievable"},
                "clarity": {"score": 7, "notes": "Some tasks need detail"}
            },
            "issues": [
                {
                    "severity": "MEDIUM",
                    "title": "Missing error handling strategy"
                },
                {
                    "severity": "LOW",
                    "title": "Clarify deployment steps"
                }
            ],
            "strengths": [
                "Good phase structure"
            ],
            "recommendations": [
                "Add error handling section",
                "Detail deployment process"
            ]
        },
        {
            "tool": "claude",
            "raw_review": "Plan review content from claude",
            "overall_score": 8.0,
            "recommendation": "APPROVE",
            "dimensions": {
                "completeness": {"score": 8, "notes": "Comprehensive coverage"},
                "feasibility": {"score": 9, "notes": "Very realistic"},
                "clarity": {"score": 8, "notes": "Well-written"}
            },
            "issues": [],
            "strengths": [
                "Excellent dependency tracking",
                "Clear success criteria"
            ],
            "recommendations": [
                "Consider adding metrics tracking"
            ]
        }
    ]


def test_plan_review_model_comparison_table(mock_plan_review_responses):
    """
    Test that plan review comparison table renders correctly with Rich Table.

    Verifies:
    - Comparison table with multiple AI model responses
    - Overall scores displayed side-by-side
    - Recommendations count for each model
    - Issue count for each model
    - Color coding based on recommendation (APPROVE=green, REVISE=yellow, REJECT=red)
    - Dimension scores displayed correctly
    """
    # Create mock plan review results similar to fidelity review structure
    review_results = {
        "spec_id": "test-plan-spec-001",
        "spec_title": "Test Plan Specification",
        "consensus": {
            "final_recommendation": "APPROVE",
            "overall_score": 8.0,
            "consensus_level": "strong",
            "dimension_scores": {
                "completeness": 8.0,
                "feasibility": 8.3,
                "clarity": 7.7
            }
        },
        "parsed_responses": mock_plan_review_responses,
        "models_consulted": len(mock_plan_review_responses)
    }

    # Create FidelityReport (will be refactored to support both review types)
    report = FidelityReport(review_results)

    # Capture console output
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # Call the model comparison rendering method
    report._print_model_comparison_table(console)

    # Get rendered output
    output = console.file.getvalue()

    # Verify comparison table header
    assert "MODEL RESPONSE COMPARISON" in output
    assert "Side-by-side comparison of all AI model assessments" in output

    # Verify model columns are created (3 models)
    assert "Model 1" in output
    assert "Model 2" in output
    assert "Model 3" in output

    # Verify metric rows exist
    assert "Metric" in output
    assert "Issues Found" in output or "Issues" in output
    assert "Recommendations" in output

    # Verify issue counts are shown (model 1: 1 issue, model 2: 2 issues, model 3: 0 issues)
    # Output should show some numeric values for issues

    # Verify recommendations are present (all models have recommendations)

    print("\n--- Plan Review Comparison Table Output ---")
    print(output)
    print("--- End Output ---\n")


def test_plan_review_dimension_scores_comparison(mock_plan_review_responses):
    """
    Test that plan review dimension scores are compared across models.

    Verifies:
    - Dimension scores (completeness, feasibility, clarity) displayed for each model
    - Scores are formatted correctly (e.g., "8/10", "9/10")
    - Missing dimensions handled gracefully
    - Notes/comments for dimensions displayed
    """
    review_results = {
        "spec_id": "test-plan-spec-002",
        "consensus": {
            "dimension_scores": {
                "completeness": 8.0,
                "feasibility": 8.3,
                "clarity": 7.7
            }
        },
        "parsed_responses": mock_plan_review_responses,
        "models_consulted": len(mock_plan_review_responses)
    }

    report = FidelityReport(review_results)
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # For now, test that the report can be created and printed without errors
    # (Dimension scores rendering may be in a different method or verbose mode)
    report._print_model_comparison_table(console)

    output = console.file.getvalue()

    # Verify table renders without crashing
    assert "MODEL RESPONSE COMPARISON" in output

    print("\n--- Plan Review Dimension Scores Output ---")
    print(output)
    print("--- End Output ---\n")


def test_plan_review_issue_aggregation(mock_plan_review_responses):
    """
    Test that plan review issues are aggregated and displayed correctly.

    Verifies:
    - Issues from all models are collected
    - Issues are grouped by severity (CRITICAL, HIGH, MEDIUM, LOW)
    - Each issue shows which models identified it
    - Issue descriptions are displayed
    - Color coding by severity level
    """
    # Aggregate issues from all responses
    all_issues = []
    for response in mock_plan_review_responses:
        for issue in response.get("issues", []):
            all_issues.append({
                "issue": issue.get("title", ""),
                "severity": issue.get("severity", "MEDIUM"),
                "flagged_by": [response.get("tool", "unknown")]
            })

    review_results = {
        "spec_id": "test-plan-spec-003",
        "consensus": {
            "all_issues": all_issues
        },
        "categorized_issues": all_issues,
        "parsed_responses": mock_plan_review_responses,
        "models_consulted": len(mock_plan_review_responses)
    }

    report = FidelityReport(review_results)
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # Test the model comparison includes issue information
    report._print_model_comparison_table(console)

    output = console.file.getvalue()

    # Verify issues are shown in some form
    assert "Issues Found" in output or "Issues" in output

    # Verify issue counts are present (numeric values)
    # Model 1 has 1 issue, Model 2 has 2 issues, Model 3 has 0 issues

    print("\n--- Plan Review Issue Aggregation Output ---")
    print(output)
    print("--- End Output ---\n")


def test_plan_review_no_responses():
    """Test plan review comparison handles case with no model responses."""
    review_results = {
        "spec_id": "test-plan-spec-004",
        "consensus": {},
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)
    console = Console(file=StringIO(), width=120)

    # Should not crash when printing with no responses
    report._print_model_comparison_table(console)

    # Output should be minimal or empty
    output = console.file.getvalue()
    assert "MODEL RESPONSE COMPARISON" not in output or output.strip() == ""
