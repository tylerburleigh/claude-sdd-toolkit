"""
Integration tests for comparison view rendering.

Tests Rich TUI rendering for fidelity review, plan review, and status report
comparison views with side-by-side layouts, tables, and panels.
"""

import pytest
import json
from io import StringIO
from unittest.mock import patch, MagicMock

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from claude_skills.sdd_fidelity_review.report import FidelityReport
from claude_skills.sdd_fidelity_review.consultation import FidelityVerdict


# =============================================================================
# Fixtures for Mock Data
# =============================================================================


@pytest.fixture
def mock_spec_data():
    """Create mock spec data for testing."""
    return {
        "spec_id": "test-spec-001",
        "title": "Test Specification",
        "hierarchy": {
            "task-1": {
                "type": "task",
                "title": "Implement authentication service",
                "status": "completed",
                "metadata": {
                    "file_path": "src/auth/service.ts",
                    "requirements": [
                        "JWT token generation",
                        "Password hashing with bcrypt",
                        "Session management"
                    ]
                }
            },
            "task-2": {
                "type": "task",
                "title": "Add input validation",
                "status": "completed",
                "metadata": {
                    "file_path": "src/utils/validators.ts",
                    "requirements": [
                        "Email format validation",
                        "Password strength check"
                    ]
                }
            }
        }
    }


@pytest.fixture
def mock_fidelity_results():
    """Create mock fidelity review results with deviations."""
    return {
        "task-1": {
            "status": "pass",
            "deviations": [],
            "findings": "Authentication service correctly implements JWT tokens and bcrypt hashing."
        },
        "task-2": {
            "status": "partial",
            "deviations": [
                {
                    "requirement": "Password strength check",
                    "expected": "Minimum 8 characters, uppercase, lowercase, number, special char",
                    "actual": "Only checks minimum 6 characters",
                    "severity": "medium",
                    "recommendation": "Strengthen password validation rules"
                }
            ],
            "findings": "Input validation partially implemented, missing some password strength checks."
        }
    }


@pytest.fixture
def mock_parsed_ai_responses():
    """Create mock parsed AI responses for comparison view."""
    return [
        {
            "model": "gemini",
            "verdict": "pass",
            "issues": [],
            "recommendations": ["Add rate limiting", "Implement 2FA support"],
            "confidence": "high"
        },
        {
            "model": "codex",
            "verdict": "partial",
            "issues": [
                {
                    "severity": "medium",
                    "description": "Password validation too weak"
                }
            ],
            "recommendations": ["Strengthen password rules"],
            "confidence": "medium"
        }
    ]


@pytest.fixture
def mock_categorized_issues():
    """Create mock categorized issues for consensus matrix testing."""
    return [
        {
            "issue": "Missing input sanitization in user registration endpoint",
            "severity": "critical",
            "category": "security",
            "agreed_by": ["gemini", "codex", "claude"],
            "agreement_count": 3
        },
        {
            "issue": "Password validation requirements too weak",
            "severity": "high",
            "category": "security",
            "agreed_by": ["codex", "claude"],
            "agreement_count": 2
        },
        {
            "issue": "Missing error handling in authentication flow",
            "severity": "medium",
            "category": "reliability",
            "agreed_by": ["gemini"],
            "agreement_count": 1
        }
    ]


# =============================================================================
# Test: Fidelity Review Side-by-Side Comparison
# =============================================================================


def test_fidelity_review_side_by_side_comparison(
    mock_spec_data,
    mock_fidelity_results,
    mock_parsed_ai_responses
):
    """
    Test that fidelity review side-by-side comparison renders correctly.

    Verifies:
    - Model comparison table is created with Rich Table
    - Verdicts are displayed side-by-side across multiple models
    - Issue counts are shown for each model
    - Recommendations are formatted correctly
    - Color coding is applied (green=PASS, red=FAIL, yellow=PARTIAL)
    """
    # Create FidelityReport with mock data in expected format
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "partial",
            "agreement_rate": 0.5,
            "consensus_issues": [],
            "consensus_recommendations": []
        },
        "categorized_issues": [],
        "parsed_responses": mock_parsed_ai_responses,
        "models_consulted": len(mock_parsed_ai_responses)
    }

    report = FidelityReport(review_results)

    # Capture console output
    console = Console(file=StringIO(), width=100, legacy_windows=False)

    # Call the internal method that renders comparison table
    report._print_model_comparison_table(console)

    # Get rendered output
    output = console.file.getvalue()

    # Verify comparison table header
    assert "MODEL RESPONSE COMPARISON" in output
    assert "Side-by-side comparison of all AI model assessments" in output

    # Verify model columns are created
    assert "Model 1" in output
    assert "Model 2" in output

    # Verify verdicts are displayed
    assert "Verdict" in output
    assert "PASS" in output or "pass" in output.lower()
    assert "PARTIAL" in output or "partial" in output.lower()

    # Verify issue count row
    assert "Issue Count" in output or "Issues" in output

    # Verify recommendations are present
    # (The output includes recommendation counts or summaries)

    print("\n--- Comparison View Output ---")
    print(output)
    print("--- End Output ---\n")


def test_fidelity_review_no_models(mock_spec_data, mock_fidelity_results):
    """Test comparison view handles case with no AI model responses."""
    # Create report with no parsed responses
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {},
        "categorized_issues": [],
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)

    # Should not crash when printing comparison
    console = Console(file=StringIO(), width=100)
    report._print_model_comparison_table(console)

    # Output should be minimal or empty
    output = console.file.getvalue()
    assert "MODEL RESPONSE COMPARISON" not in output or output.strip() == ""


# =============================================================================
# Test: Consensus Matrix Display
# =============================================================================


def test_consensus_matrix_display(
    mock_parsed_ai_responses,
    mock_categorized_issues
):
    """
    Test that consensus matrix renders correctly with Rich Table.

    Verifies:
    - Consensus matrix table is created with issue rows and model columns
    - Each model column shows checkmark (‚úì) or dash (‚Äî) for agreement
    - Agreement percentage is calculated and displayed
    - Issues are color-coded by severity (critical=red, high=yellow, medium=blue, low=cyan)
    - Matrix shows which models identified each issue
    """
    # Create FidelityReport with mock data
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "partial",
            "agreement_rate": 0.67
        },
        "categorized_issues": mock_categorized_issues,
        "parsed_responses": mock_parsed_ai_responses,
        "models_consulted": len(mock_parsed_ai_responses)
    }

    report = FidelityReport(review_results)

    # Capture console output
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # Call the consensus matrix rendering method
    report._print_consensus_matrix(console, mock_categorized_issues)

    # Get rendered output
    output = console.file.getvalue()

    # Verify consensus matrix header
    assert "CONSENSUS MATRIX" in output
    assert "Shows which AI models identified each issue" in output

    # Verify column headers
    assert "Issue" in output
    assert "M1" in output  # Model 1 column
    assert "M2" in output  # Model 2 column
    assert "Agreement" in output

    # Verify issues are present (at least some text from issues)
    assert "sanitization" in output or "validation" in output or "error" in output

    # Verify agreement indicators (checkmarks or dashes)
    assert "‚úì" in output or "‚Äî" in output

    # Verify agreement percentages are shown
    assert "%" in output

    print("\n--- Consensus Matrix Output ---")
    print(output)
    print("--- End Output ---\n")


def test_consensus_matrix_no_issues(mock_parsed_ai_responses):
    """Test consensus matrix handles case with no categorized issues."""
    # Create report with no issues
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {},
        "categorized_issues": [],
        "parsed_responses": mock_parsed_ai_responses,
        "models_consulted": len(mock_parsed_ai_responses)
    }

    report = FidelityReport(review_results)

    # Should not crash when printing matrix with no issues
    console = Console(file=StringIO(), width=120)
    report._print_consensus_matrix(console, [])

    # Output should have header but no issue rows
    output = console.file.getvalue()
    # Matrix might still print header or be empty
    # The important thing is it doesn't crash


# =============================================================================
# Test: Issue Severity Panel Rendering
# =============================================================================


def test_issue_severity_panel_rendering(mock_categorized_issues, capsys):
    """
    Test that issue severity panels render correctly with Rich Panel components.

    Verifies:
    - Issues grouped by severity (critical, high, medium, low)
    - Each severity has its own color-coded panel (red, yellow, blue, cyan)
    - Panel titles include severity name and count
    - Panel icons are displayed (üî¥, üü°, üîµ, ‚ö™)
    - Issues are displayed as bullet points within panels
    - Border styles match severity colors
    """
    # Create FidelityReport with categorized issues
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "partial",
            "agreement_rate": 0.67,
            "consensus_recommendations": ["Improve input validation"]
        },
        "categorized_issues": mock_categorized_issues,
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)

    # Call print_console_rich() which prints to stdout
    # We'll capture it with capsys
    report.print_console_rich(verbose=False)

    # Capture stdout
    captured = capsys.readouterr()
    output = captured.out

    # Verify report header
    assert "IMPLEMENTATION FIDELITY REVIEW" in output
    assert "test-spec-001" in output

    # Verify consensus verdict is shown
    assert "Consensus Verdict" in output
    assert "Agreement Rate" in output

    # Verify severity panel titles with icons
    assert "CRITICAL ISSUES" in output or "critical" in output.lower()
    assert "HIGH" in output or "high" in output.lower()
    assert "MEDIUM" in output or "medium" in output.lower()

    # Verify issue content is present
    assert "sanitization" in output or "validation" in output or "error" in output

    # Verify bullet points for issues
    assert "‚Ä¢" in output


def test_issue_severity_panel_empty(capsys):
    """Test severity panels handle case with no issues."""
    # Create report with no categorized issues
    review_results = {
        "spec_id": "test-spec-001",
        "consensus": {
            "consensus_verdict": "pass",
            "agreement_rate": 1.0,
            "consensus_recommendations": []
        },
        "categorized_issues": [],
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)

    # Should not crash when rendering with no issues
    report.print_console_rich(verbose=False)

    # Capture stdout
    captured = capsys.readouterr()
    output = captured.out

    # Verify header still renders
    assert "IMPLEMENTATION FIDELITY REVIEW" in output

    # Verify verdict is shown even without issues
    assert "Consensus Verdict" in output
    assert "PASS" in output or "pass" in output.lower()


# =============================================================================
# Test: Plan Review Comparison Tables
# =============================================================================


@pytest.fixture
def mock_plan_review_responses():
    """Create mock plan review responses for comparison testing."""
    return [
        {
            "tool": "gemini",
            "raw_review": "Plan review content from gemini",
            "overall_score": 8.5,
            "recommendation": "APPROVE",
            "dimensions": {
                "completeness": {"score": 9, "notes": "Well-structured phases"},
                "feasibility": {"score": 8, "notes": "Realistic timeline"},
                "clarity": {"score": 8, "notes": "Clear task descriptions"}
            },
            "issues": [
                {
                    "severity": "LOW",
                    "title": "Add more test coverage requirements"
                }
            ],
            "strengths": [
                "Well-defined phases",
                "Clear dependencies"
            ],
            "recommendations": [
                "Add performance benchmarks",
                "Include rollback plan"
            ]
        },
        {
            "tool": "codex",
            "raw_review": "Plan review content from codex",
            "overall_score": 7.5,
            "recommendation": "REVISE",
            "dimensions": {
                "completeness": {"score": 7, "notes": "Missing error handling"},
                "feasibility": {"score": 8, "notes": "Timeline is achievable"},
                "clarity": {"score": 7, "notes": "Some tasks need detail"}
            },
            "issues": [
                {
                    "severity": "MEDIUM",
                    "title": "Missing error handling strategy"
                },
                {
                    "severity": "LOW",
                    "title": "Clarify deployment steps"
                }
            ],
            "strengths": [
                "Good phase structure"
            ],
            "recommendations": [
                "Add error handling section",
                "Detail deployment process"
            ]
        },
        {
            "tool": "claude",
            "raw_review": "Plan review content from claude",
            "overall_score": 8.0,
            "recommendation": "APPROVE",
            "dimensions": {
                "completeness": {"score": 8, "notes": "Comprehensive coverage"},
                "feasibility": {"score": 9, "notes": "Very realistic"},
                "clarity": {"score": 8, "notes": "Well-written"}
            },
            "issues": [],
            "strengths": [
                "Excellent dependency tracking",
                "Clear success criteria"
            ],
            "recommendations": [
                "Consider adding metrics tracking"
            ]
        }
    ]


def test_plan_review_model_comparison_table(mock_plan_review_responses):
    """
    Test that plan review comparison table renders correctly with Rich Table.

    Verifies:
    - Comparison table with multiple AI model responses
    - Overall scores displayed side-by-side
    - Recommendations count for each model
    - Issue count for each model
    - Color coding based on recommendation (APPROVE=green, REVISE=yellow, REJECT=red)
    - Dimension scores displayed correctly
    """
    # Create mock plan review results similar to fidelity review structure
    review_results = {
        "spec_id": "test-plan-spec-001",
        "spec_title": "Test Plan Specification",
        "consensus": {
            "final_recommendation": "APPROVE",
            "overall_score": 8.0,
            "consensus_level": "strong",
            "dimension_scores": {
                "completeness": 8.0,
                "feasibility": 8.3,
                "clarity": 7.7
            }
        },
        "parsed_responses": mock_plan_review_responses,
        "models_consulted": len(mock_plan_review_responses)
    }

    # Create FidelityReport (will be refactored to support both review types)
    report = FidelityReport(review_results)

    # Capture console output
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # Call the model comparison rendering method
    report._print_model_comparison_table(console)

    # Get rendered output
    output = console.file.getvalue()

    # Verify comparison table header
    assert "MODEL RESPONSE COMPARISON" in output
    assert "Side-by-side comparison of all AI model assessments" in output

    # Verify model columns are created (3 models)
    assert "Model 1" in output
    assert "Model 2" in output
    assert "Model 3" in output

    # Verify metric rows exist
    assert "Metric" in output
    assert "Issues Found" in output or "Issues" in output
    assert "Recommendations" in output

    # Verify issue counts are shown (model 1: 1 issue, model 2: 2 issues, model 3: 0 issues)
    # Output should show some numeric values for issues

    # Verify recommendations are present (all models have recommendations)

    print("\n--- Plan Review Comparison Table Output ---")
    print(output)
    print("--- End Output ---\n")


def test_plan_review_dimension_scores_comparison(mock_plan_review_responses):
    """
    Test that plan review dimension scores are compared across models.

    Verifies:
    - Dimension scores (completeness, feasibility, clarity) displayed for each model
    - Scores are formatted correctly (e.g., "8/10", "9/10")
    - Missing dimensions handled gracefully
    - Notes/comments for dimensions displayed
    """
    review_results = {
        "spec_id": "test-plan-spec-002",
        "consensus": {
            "dimension_scores": {
                "completeness": 8.0,
                "feasibility": 8.3,
                "clarity": 7.7
            }
        },
        "parsed_responses": mock_plan_review_responses,
        "models_consulted": len(mock_plan_review_responses)
    }

    report = FidelityReport(review_results)
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # For now, test that the report can be created and printed without errors
    # (Dimension scores rendering may be in a different method or verbose mode)
    report._print_model_comparison_table(console)

    output = console.file.getvalue()

    # Verify table renders without crashing
    assert "MODEL RESPONSE COMPARISON" in output

    print("\n--- Plan Review Dimension Scores Output ---")
    print(output)
    print("--- End Output ---\n")


def test_plan_review_issue_aggregation(mock_plan_review_responses):
    """
    Test that plan review issues are aggregated and displayed correctly.

    Verifies:
    - Issues from all models are collected
    - Issues are grouped by severity (CRITICAL, HIGH, MEDIUM, LOW)
    - Each issue shows which models identified it
    - Issue descriptions are displayed
    - Color coding by severity level
    """
    # Aggregate issues from all responses
    all_issues = []
    for response in mock_plan_review_responses:
        for issue in response.get("issues", []):
            all_issues.append({
                "issue": issue.get("title", ""),
                "severity": issue.get("severity", "MEDIUM"),
                "flagged_by": [response.get("tool", "unknown")]
            })

    review_results = {
        "spec_id": "test-plan-spec-003",
        "consensus": {
            "all_issues": all_issues
        },
        "categorized_issues": all_issues,
        "parsed_responses": mock_plan_review_responses,
        "models_consulted": len(mock_plan_review_responses)
    }

    report = FidelityReport(review_results)
    console = Console(file=StringIO(), width=120, legacy_windows=False)

    # Test the model comparison includes issue information
    report._print_model_comparison_table(console)

    output = console.file.getvalue()

    # Verify issues are shown in some form
    assert "Issues Found" in output or "Issues" in output

    # Verify issue counts are present (numeric values)
    # Model 1 has 1 issue, Model 2 has 2 issues, Model 3 has 0 issues

    print("\n--- Plan Review Issue Aggregation Output ---")
    print(output)
    print("--- End Output ---\n")


def test_plan_review_no_responses():
    """Test plan review comparison handles case with no model responses."""
    review_results = {
        "spec_id": "test-plan-spec-004",
        "consensus": {},
        "parsed_responses": [],
        "models_consulted": 0
    }

    report = FidelityReport(review_results)
    console = Console(file=StringIO(), width=120)

    # Should not crash when printing with no responses
    report._print_model_comparison_table(console)

    # Output should be minimal or empty
    output = console.file.getvalue()
    assert "MODEL RESPONSE COMPARISON" not in output or output.strip() == ""


# =============================================================================
# Test: Status Report Dashboard Layout
# =============================================================================


@pytest.fixture
def mock_dashboard_spec_data():
    """Create mock spec data for dashboard testing."""
    return {
        "spec_id": "test-dashboard-001",
        "title": "Dashboard Test Specification",
        "hierarchy": {
            "phase-1": {
                "type": "phase",
                "title": "Phase 1: Setup",
                "status": "completed",
                "total_tasks": 3,
                "completed_tasks": 3
            },
            "phase-2": {
                "type": "phase",
                "title": "Phase 2: Core Features",
                "status": "in_progress",
                "total_tasks": 8,
                "completed_tasks": 5
            },
            "phase-3": {
                "type": "phase",
                "title": "Phase 3: Polish",
                "status": "pending",
                "total_tasks": 4,
                "completed_tasks": 0
            },
            "task-1-1": {
                "type": "task",
                "title": "Setup project structure",
                "status": "completed"
            },
            "task-1-2": {
                "type": "task",
                "title": "Configure dependencies",
                "status": "completed"
            },
            "task-1-3": {
                "type": "task",
                "title": "Create initial README",
                "status": "completed"
            },
            "task-2-1": {
                "type": "task",
                "title": "Implement authentication",
                "status": "completed"
            },
            "task-2-2": {
                "type": "task",
                "title": "Add database layer",
                "status": "completed"
            },
            "task-2-3": {
                "type": "task",
                "title": "Build API endpoints",
                "status": "in_progress"
            },
            "task-2-4": {
                "type": "task",
                "title": "Add rate limiting",
                "status": "blocked",
                "metadata": {
                    "blocker_reason": "Waiting for Redis setup"
                },
                "dependencies": {
                    "blocked_by": ["external-dependency"]
                }
            }
        }
    }


def test_dashboard_layout_structure(mock_dashboard_spec_data):
    """
    Test that status report dashboard layout renders with correct structure.

    Verifies:
    - Layout has multiple panels (phases, progress, blockers)
    - Layout is split into top and bottom sections
    - Top section has left (phases) and right (progress) panels
    - Bottom section contains blockers panel
    - All panels render without errors
    """
    from claude_skills.sdd_update.status_report import create_status_layout

    # Create layout
    layout = create_status_layout(mock_dashboard_spec_data)

    # Verify layout structure
    assert layout is not None
    assert hasattr(layout, "children")
    assert len(layout.children) > 0

    # Verify layout can be rendered
    console = Console(file=StringIO(), width=120, legacy_windows=False)
    console.print(layout)

    output = console.file.getvalue()

    # Verify key sections are present
    assert "Phases" in output or "Phase" in output
    assert "Progress" in output
    assert "Blockers" in output or "Blocker" in output

    print("\n--- Dashboard Layout Output ---")
    print(output)
    print("--- End Output ---\n")


def test_dashboard_phases_panel_rendering(mock_dashboard_spec_data):
    """
    Test that phases panel renders correctly in dashboard.

    Verifies:
    - Phase names are displayed
    - Phase status indicators shown (‚úì, ‚óè, ‚óã)
    - Progress bars are rendered
    - Completion percentages displayed
    - Color coding by status (green=completed, yellow=in_progress, dim=pending)
    """
    from claude_skills.sdd_update.status_report import create_phases_panel

    # Create phases panel
    panel = create_phases_panel(mock_dashboard_spec_data)

    # Render to string
    console = Console(file=StringIO(), width=120, legacy_windows=False)
    console.print(panel)

    output = console.file.getvalue()

    # Verify panel title
    assert "Phases" in output

    # Verify phase titles
    assert "Setup" in output or "Phase 1" in output
    assert "Core Features" in output or "Phase 2" in output
    assert "Polish" in output or "Phase 3" in output

    # Verify status indicators
    assert "Complete" in output or "‚úì" in output  # Phase 1 completed
    assert "In Progress" in output or "‚óè" in output  # Phase 2 in progress
    assert "Pending" in output or "‚óã" in output  # Phase 3 pending

    # Verify progress percentages
    assert "100%" in output  # Phase 1: 3/3 = 100%
    assert "62%" in output or "63%" in output  # Phase 2: 5/8 = 62.5%
    assert "0%" in output  # Phase 3: 0/4 = 0%

    print("\n--- Phases Panel Output ---")
    print(output)
    print("--- End Output ---\n")


def test_dashboard_progress_panel_metrics(mock_dashboard_spec_data):
    """
    Test that progress panel shows correct metrics.

    Verifies:
    - Total tasks count
    - Completed tasks count
    - In-progress tasks count
    - Blocked tasks count
    - Overall completion percentage
    - Visual progress bar
    """
    from claude_skills.sdd_update.status_report import create_progress_panel

    # Create progress panel
    panel = create_progress_panel(mock_dashboard_spec_data)

    # Render to string
    console = Console(file=StringIO(), width=120, legacy_windows=False)
    console.print(panel)

    output = console.file.getvalue()

    # Verify panel title
    assert "Progress" in output

    # Verify metrics are shown
    assert "Overall" in output or "Total" in output
    assert "Completed" in output
    assert "In Progress" in output
    assert "Blocked" in output or "Remaining" in output

    # Should have numeric values
    # 7 tasks total (task-1-1 through task-2-4, excluding phases)
    # 5 completed (task-1-1, 1-2, 1-3, 2-1, 2-2)
    # 1 in progress (task-2-3)
    # 1 blocked (task-2-4)

    print("\n--- Progress Panel Output ---")
    print(output)
    print("--- End Output ---\n")


def test_dashboard_blockers_panel_display(mock_dashboard_spec_data):
    """
    Test that blockers panel displays blocked tasks correctly.

    Verifies:
    - Blocked task IDs shown
    - Blocked task titles displayed
    - Blocker reasons included
    - Dependency information shown
    - Color coding for blockers (red/yellow)
    """
    from claude_skills.sdd_update.status_report import create_blockers_panel

    # Create blockers panel
    panel = create_blockers_panel(mock_dashboard_spec_data)

    # Render to string
    console = Console(file=StringIO(), width=120, legacy_windows=False)
    console.print(panel)

    output = console.file.getvalue()

    # Verify panel title
    assert "Blockers" in output or "Blocker" in output

    # Verify blocked task information
    assert "task-2-4" in output
    assert "rate limiting" in output.lower() or "Rate limiting" in output
    assert "Redis" in output or "redis" in output  # Blocker reason

    print("\n--- Blockers Panel Output ---")
    print(output)
    print("--- End Output ---\n")


def test_dashboard_empty_spec():
    """Test dashboard handles empty spec data gracefully."""
    from claude_skills.sdd_update.status_report import create_status_layout

    empty_spec = {
        "spec_id": "empty-spec",
        "hierarchy": {}
    }

    # Should not crash with empty spec
    layout = create_status_layout(empty_spec)

    console = Console(file=StringIO(), width=120)
    console.print(layout)

    output = console.file.getvalue()

    # Should still render structure
    assert "Phases" in output or "Progress" in output
    # May show empty state messages


def test_dashboard_no_blockers():
    """Test dashboard when there are no blocked tasks."""
    from claude_skills.sdd_update.status_report import create_blockers_panel

    spec_without_blockers = {
        "hierarchy": {
            "task-1": {
                "type": "task",
                "title": "Task 1",
                "status": "completed"
            },
            "task-2": {
                "type": "task",
                "title": "Task 2",
                "status": "in_progress"
            }
        }
    }

    panel = create_blockers_panel(spec_without_blockers)

    console = Console(file=StringIO(), width=120)
    console.print(panel)

    output = console.file.getvalue()

    # Should show "no blockers" message
    assert "No blockers" in output or "no blockers" in output


def test_dashboard_status_summary_integration(mock_dashboard_spec_data):
    """
    Test get_status_summary provides correct data for dashboard.

    Verifies:
    - Summary dictionary structure
    - Correct task counts
    - Phase data included
    - Blocker information present
    """
    from claude_skills.sdd_update.status_report import get_status_summary

    summary = get_status_summary(mock_dashboard_spec_data)

    # Verify structure
    assert isinstance(summary, dict)
    assert "total_tasks" in summary
    assert "completed_tasks" in summary
    assert "in_progress_tasks" in summary
    assert "blocked_tasks" in summary
    assert "phases" in summary
    assert "blockers" in summary

    # Verify counts (7 tasks total: task-1-1, 1-2, 1-3, 2-1, 2-2, 2-3, 2-4)
    assert summary["total_tasks"] == 7
    assert summary["completed_tasks"] == 5  # task-1-1, 1-2, 1-3, 2-1, 2-2
    assert summary["in_progress_tasks"] == 1  # task-2-3
    assert summary["blocked_tasks"] == 1  # task-2-4

    # Verify phases data
    assert len(summary["phases"]) == 3
    assert summary["phases"][0]["id"] == "phase-1"
    assert summary["phases"][0]["status"] == "completed"

    # Verify blockers data
    assert len(summary["blockers"]) == 1
    assert summary["blockers"][0]["id"] == "task-2-4"
    assert "Redis" in summary["blockers"][0]["reason"]
